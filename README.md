# From Q-Learning to Deep Reinforcement learning: Pendulum, Cartpole and Breakout

Over the past few weeks, I delved into implementing and analyzing reinforcement learning algorithms across three diverse environments: Pendulum, CartPole, and Breakout. Here's a glimpse into the journey:

🎯 Pendulum: Tackling a continuous control problem using Q-learning. By discretizing the state and action spaces, I demonstrated that even traditional tabular methods can excel in simplified continuous environments. The result? A stable pendulum through adaptive strategies.

🛠️ CartPole: Using Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO), I designed a custom ANN to balance the pole with precision. Both models achieved the maximum reward, showcasing the power of deep RL in environments demanding quick and reliable decisions.

🎮 Breakout: A step into visual RL with a custom Convolutional Neural Network (CNN) to process high-dimensional image inputs. While DQN showed aggressive, high-reward strategies, PPO maintained consistent, stable play. This reinforced the importance of extended training and robust architectures in complex, image-based tasks.

💡 Key Takeaways:
- Tailoring algorithms to the environment is critical.
- Simpler methods like Q-learning can still shine with creative adaptations.
- Deep RL thrives in structured, high-dimensional spaces but demands computational investment.

This project not only deepened my understanding of RL but also highlighted its versatility—from controlling a pendulum to strategizing in Atari games. 
